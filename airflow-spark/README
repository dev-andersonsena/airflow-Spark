
Instalar a versão 3.9 do Python = sudo apt install python3.9
Instalar o módulo de virtual enviroment da versão 3.9 do python = sudo apt install python3.9-venv





INSTALAR AMBIENTE VIRTUAL

1. python3.9 -m venv ambientevirt
2. source ambientevirt/bin/activate


PIP INSTALL 
pip install requests==2.27.1
pip install --upgrade pip
pip install apache-airflow-providers-apache-spark

INSTALAÇÃO DO AIRFLOW
AIRFLOW_VERSION=2.3.2
PYTHON_VERSION=3.9
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
pip install "apache-airflow[postgres,celery,redis]==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
export AIRFLOW_HOME=$(pwd)/airflow_pipeline
export SPARK_HOME=/mnt/c/Users/ander/Desktop/airflow-spark/airflow-spark/curso2/spark-3.1.3-bin-hadoop3.2
airflow standalone


TER O JAVA INSTALADO

java -version
apt-get install openjdk-8-jdk-headless -qq

INSTALAR O PYSPARK

pip install pyspark==3.3.1

SPARK SUBMIT BAIXAR

wget https://archive.apache.org/dist/spark/spark-3.1.3/spark-3.1.3-bin-hadoop3.2.tgz
tar -xvzf spark-3.1.3-bin-hadoop3.2.tgz